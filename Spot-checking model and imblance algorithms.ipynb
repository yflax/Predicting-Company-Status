{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    ***                                                                                 Main Project Notebook    ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotchecking Models :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = ['funding_rounds', 'milestones', 'relationships', 'tracking_interval_days']\n",
    "categorical_features = ['continent', 'funding_total_bin']\n",
    "\n",
    "# Create transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the RandomForestClassifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', classifier)])\n",
    "\n",
    "\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(random_state=43, class_weight= 'balanced' )\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = ['funding_rounds', 'milestones', 'relationships', 'tracking_interval_days']\n",
    "categorical_features = ['continent', 'funding_total_bin']\n",
    "\n",
    "# Create transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X2_train, y2_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred_binary_classif = pipeline.predict(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Bagging with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Assuming you have defined X and y\n",
    "# Split your data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Specify categorical and numerical feature columns\n",
    "categorical_features = ['continent', 'high_level_category']\n",
    "numerical_features = ['funding_rounds', 'milestones', 'relationships', 'product_count', 'office_count', 'Company_degree_count', 'funding_total_usd']\n",
    "\n",
    "# Create a column transformer with OneHotEncoder and StandardScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other features as they are\n",
    ")\n",
    "\n",
    "# Create the SMOTE-Bagging classifier\n",
    "smote_bagging = BalancedBaggingClassifier(sampler=SMOTE())\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('model', smote_bagging)\n",
    "])\n",
    "\n",
    "# Perform cross-validation on the training set\n",
    "#cv_results = cross_validate(pipeline, X_train_encoded, y_train, scoring=\"balanced_accuracy\")\n",
    "\n",
    "# Print the mean and standard deviation of cross-validation results\n",
    "#print(f\"Cross-Validation Balanced Accuracy: {cv_results['test_score'].mean():.3f} +/- {cv_results['test_score'].std():.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the pipeline on the entire training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the separate test set\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"Test Set Balanced Accuracy: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Histogram Boosting Classifier with Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Define the columns to encode and scale\n",
    "columns_to_encode = ['continent', 'high_level_category']\n",
    "columns_to_scale = ['funding_rounds', 'milestones', 'relationships',\n",
    "       'product_count', 'office_count','Company_degree_count', 'funding_total_usd', 'tracking_interval_days']  \n",
    "\n",
    "# Create a column transformer with OneHotEncoder, StandardScaler, and other transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), columns_to_encode),\n",
    "        ('num', StandardScaler(), columns_to_scale)\n",
    "        \n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "# Apply RandomUnderSampler to the training data\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "# Create a pipeline with preprocessing, resampling, and model\n",
    "model = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('resample', rus),\n",
    "    ('classifier', HistGradientBoostingClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also tried Adasyn with GradientHistogramBoosting wich had the least favourable results for G-Mean , so excluded it from this notebook.\n",
    "# tested the above models tweaking the features each time to include different subsets \n",
    "# best feature set was with GHist + UNderSampling , but did not attempt every possible permutation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
