{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering for Objects Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean Objects DataFrame\n",
    "- Create new , significant features \n",
    "- OneHotEncode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by removing redundant columns \n",
    "list(objects2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['twitter_username',\n",
    " 'logo_url', ]\n",
    "\n",
    "objects2 = objects2.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need one column for the location , find the best one to OneHotEncode\n",
    "loc_cols = [ 'country_code','state_code','city','region']\n",
    "objects2[loc_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region is definitely too specific for OHC , but let's explore distinct value count in other columns\n",
    "objects2['country_code'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects2['state_code'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects2['city'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state_code has the least distinct values because it only rpovides a value for countries with states\n",
    "-  best option is so aggregate country_code into high level continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects2['country_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These codes are part of the ISO 3166-1 standard and are commonly used to uniquely identify countries and territories \n",
    "Credits to https://statisticstimes.com/geography/countries-by-continents.php for the excel file for mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\Admin\\Final-Project\\ISO Continents.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map=['ISO-alpha3 Code','Continent']\n",
    "mapping =df[map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects3 = objects2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_list = []\n",
    "\n",
    "for code in objects3['country_code']:\n",
    "    if code in df['ISO-alpha3 Code'].values:\n",
    "        region = df.loc[df['ISO-alpha3 Code'] == code, 'Continent'].values[0]\n",
    "        continent_list.append(region)\n",
    "    else:\n",
    "        continent_list.append(None)\n",
    "\n",
    "objects3['continent'] = continent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects3['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects3.to_csv('objects3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects4= objects3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(objects4['category_code'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with only location info , then isolate for NaN continent rows so can see if can impute . if no , drop these columns or categorize as 'virtual'\n",
    "locationinfo = ['country_code', 'state_code', 'city', 'region','continent']\n",
    "locationsdf = objects3[locationinfo]\n",
    "locationsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring NaN continent rows\n",
    "no_continent = locationsdf[locationsdf['continent'].isna()]\n",
    "no_continent['city'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_continent['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore rows with unknown regions\n",
    "no_region = no_continent[no_continent['region']=='unknown']\n",
    "no_region['city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its less than one percent of the data ,so i think I should skip the work for now because of time constraint.  \n",
    "Now , let's assume NaN continents for the most part simply do not have a location / HQs . So , rewrite those businesses as 'Virtual'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects5 = objects4.copy()\n",
    "\n",
    "# Replace NaN values in the 'continent' column with 'Virtual'\n",
    "objects5['continent'].fillna('Virtual', inplace=True)\n",
    "\n",
    "# validate\n",
    "objects5['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "objects5['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = ['updated_at', 'founded_at']\n",
    "intervaldf = objects5[ intervals]\n",
    "intervaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(intervaldf['founded_at'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created year is at shows a year (  if not the first year) in which the company was in existence so gives some idea of company's age although not 100% accurate\n",
    "intervals3 = ['created_at', 'updated_at']\n",
    "intervaldf3 = objects5[ intervals3]\n",
    "intervaldf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIrst , convert both columns to DateTIme\n",
    "objects5['created_at'] = pd.to_datetime(objects5['created_at'])\n",
    "objects5['updated_at']= pd.to_datetime(objects5['updated_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createing company tracking interval column\n",
    "objects5['tracking_interval'] = objects5['updated_at'] - objects5['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate \n",
    "objects5['tracking_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what kind of startups are under Person\n",
    "person_type = objects[objects['entity_type']== 'Person']\n",
    "person_type.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove all persons becasue theyre not startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects6 = objects5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects6 = objects6[objects6['entity_type'] != 'Person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what kind of startups are under Peroduct\n",
    "prod_type = objects[objects['entity_type']== 'Product']\n",
    "prod_type.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if products represent individual offerings of startups and therfore do not represent companies/startups themselves and therefore should be dropped\n",
    "# if total funding is 0 , indicates that its not a a company\n",
    "prod_type['funding_total_usd'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check further\n",
    "prod_type.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "category_code , description , short_description all completely null , while parent_id is almost completely full . Perhaps parent_Id represents the actual company behind the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check parent_id values\n",
    "prod_type['parent_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis is true . Product does not represent a type of startup , rather it represents the offering/product of the startups , and parent_id links it to its associated company.\n",
    "Separate Products into a different dataframe to reference after MVP . \n",
    "For now , we will only work with the companies themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_type.to_csv('products.csv')\n",
    "com_type.to_csv('companies6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_type = objects6[objects6['entity_type']== 'Company']\n",
    "com_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_com_type = com_type.sort_values(by='updated_at', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "sorted_com_type.head(3) # just want to see the most recent updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'founded_at' column to datetime if it's not already\n",
    "com_type['founded_at'] = pd.to_datetime(com_type['founded_at'], errors='coerce')\n",
    "\n",
    "# Extract the year from 'founded_at' column\n",
    "com_type['founded_year'] = com_type['founded_at'].dt.year\n",
    "\n",
    "# Calculate the mean of the founded years\n",
    "average_founded_year = com_type['founded_year'].mean()\n",
    "\n",
    "# Print the calculated mean\n",
    "print(\"Average Founded Year:\", average_founded_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(com_type['founded_year'].notna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91227 out of 196553 have founding date values . \n",
    "conduct baseline model on all rows ie. removing founding dates , and then after only with those 90K rows and see if startup age (hypothesis 11.5 = closed) changes prediction score\n",
    "\n",
    "-- now create model df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the baseline features \n",
    "columns = [ 'funding_rounds',\n",
    " 'funding_total_usd','continent','tracking_interval','milestones','relationships', 'status']\n",
    "model1 = com_type[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the status column into target classes \n",
    "model1['status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy for imbalanced target class 'operating' ( which makes empirical sense since the tracking range is not long enough to capture the average lifespan of a startup):\n",
    ">  Options:\n",
    "- aggregate acquired and ipo into one target class 'Exit' since that's the goal for VCs  ( adjust loss function accordingly ie. false negatives for thsi class more punished)\n",
    "- Resampling:\n",
    "\n",
    "Oversampling: Increase the number of instances in the minority class by duplicating or creating synthetic samples. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be effective.\n",
    "Undersampling: Reduce the number of instances in the majority class by randomly selecting a subset of samples. This can help balance the class distribution.\n",
    "Weighted Loss Function:\n",
    "\n",
    "- Adjust the loss function in your classification algorithm to give more weight to the minority class. This can make the model more sensitive to the minority class during training.\n",
    "Ensemble Methods:\n",
    "\n",
    "- Use ensemble techniques like Random Forest, which can handle imbalanced classes better than individual models. Random Forest's construction of multiple decision trees can mitigate the class imbalance issue.\n",
    "Anomaly Detection:\n",
    "\n",
    "- Treat the minority class as an anomaly detection problem. This involves building a model to identify instances that deviate significantly from the majority class. Techniques like Isolation Forest and One-Class SVM can be useful.\n",
    "Change the Evaluation Metric:\n",
    "\n",
    "- Instead of using accuracy as the evaluation metric, consider metrics like precision, recall, F1-score, or area under the ROC curve (AUC-ROC) that provide a more comprehensive understanding of model performance for imbalanced classes.\n",
    "- Model Selection:\n",
    "\n",
    "Choose algorithms that inherently handle imbalanced data well, such as algorithms that allow you to set class weights or algorithms designed to work well with skewed distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# successful exit \n",
    "model2 = model1.copy()\n",
    "\n",
    "# Create a new column 'successful_exit' based on 'status' conditions\n",
    "model2['successful_exit'] = model2['status'].apply(lambda x: 'Exit' if x in ['acquired', 'ipo'] else x)\n",
    "\n",
    "#validate\n",
    "model2['successful_exit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Exploratory Data Analysis: In-Depth Investigation and Distributions of Model Features\n",
    "analysis goes beyond the initial EDA and delves deeper into specific aspects of the data, particularly focusing on features relevant to the  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 .describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \" 90% of startups fail\" -  https://businessdatalist.com/how-long-does-a-startup-company-last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if research holds true for model features\n",
    "\n",
    "# True or False for out data?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class distribution\n",
    "class_counts = {\n",
    "    'operating': 183441,\n",
    "    'Exit': 10528,\n",
    "    'closed': 2584\n",
    "}\n",
    "\n",
    "# Calculate the total number of instances\n",
    "total_instances = sum(class_counts.values())\n",
    "\n",
    "# Calculate the ratio of 'closed' class to all other classes\n",
    "closed_ratio = class_counts['closed'] / total_instances\n",
    "\n",
    "# Plotting\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Print the ratio\n",
    "print(f'Ratio of \"closed\" class to all other classes: {closed_ratio:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False . Since this data is only a short tracking interval , only 1.3% of the startups failed. \n",
    "When researched some of these companies current status - many have shut down / bankrupt since bringing the percentage of failed companies closer to the  90% ratio , but still not at all there.\n",
    "This could be because while 90% (roughly) of startups fail , only .05% receive VC funding . This select group may not be subject to the same failure rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only numeric features from your DataFrame\n",
    "num = model3.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = num.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "\n",
    "# Set the title of the heatmap\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makes  sense . the more tracking days , the more funding rounds and milestones the dataset will have captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Distribution of numeric features\n",
    "fig_numeric = px.histogram(model3, x=\"funding_rounds\", title=\"Distribution of Funding Rounds\")\n",
    "\n",
    "# Distribution of funding categories\n",
    "fig_funding = px.histogram(model3, x=\"funding_total_bin\", title=\"Distribution of Funding Categories\")\n",
    "\n",
    "# Show the interactive plots\n",
    "fig_numeric.show()\n",
    "fig_funding.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_continents = px.histogram(model3, x=\"continent\", title=\"Distribution of Continents\")\n",
    "fig_continents.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_cities = px.histogram(objects, x=\"city\", title=\"Distribution of Cities\")\n",
    "fig_cities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_category_codes = px.histogram(objects, x=\"category_code\", title=\"Distribution of Category Codes\")\n",
    "fig_category_codes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_employee_count = px.histogram(model3, x=\"relationships\", title=\"Distribution of Employee Counts\")\n",
    "fig_employee_count.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create categorical bins for funding amount since many unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Define custom function for assigning funding categories\n",
    "def assign_funding_category(funding_total):\n",
    "    if funding_total == 0:\n",
    "        return 'Undisclosed Funding'\n",
    "    elif 0 < funding_total <= low_funding:\n",
    "        return 'Low Funding'\n",
    "    elif low_funding < funding_total <= medium_funding:\n",
    "        return 'Medium Funding'\n",
    "    elif medium_funding < funding_total <= high_funding:\n",
    "        return 'High Funding'\n",
    "\n",
    "# Calculate mean and standard deviation of funding_total_usd\n",
    "mean_funding = model2['funding_total_usd'].mean()\n",
    "std_funding = model2['funding_total_usd'].std()\n",
    "\n",
    "low_funding = mean_funding - std_funding\n",
    "medium_funding = mean_funding\n",
    "high_funding = model2['funding_total_usd'].max()\n",
    "\n",
    "# Apply the custom function to create funding categories\n",
    "model2['funding_total_bin'] = model2['funding_total_usd'].apply(assign_funding_category)\n",
    "\n",
    "# validate\n",
    "model2['funding_total_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'tracking_interval' to total days\n",
    "model2['tracking_interval_days'] = model2['tracking_interval'].dt.total_seconds() / (60 * 60 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = model3.drop(columns=['successful_exit', 'Unnamed: 0'])\n",
    "y = model3['successful_exit']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
